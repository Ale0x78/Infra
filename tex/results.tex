\section{Results (4-5 pages)} 
\label{sec:results}

Crawling the phishing pages for a total of \totalDaysCrawled{} days, we process a little over \totalURLs{} distinct urls (from \totalDomains{} distinct domains). 
Meanwhile, kitphishr identified \numberKits{} zipfiles which we de-duplicate into \numberKitFamilies{} kits from \numberofGTDomains{} with an average of \numberofGTDomainperKit{} domains per kit ($\sigma{}=\numberofGTDomainperKitSTD{}$ and a max of 42) Since we require at least 2 domains from a kit to be able to cluster it, and for the domain to execute at least 4 browser APIs and property accesses, we further filter the kits to \totalKitsToCluster{} kits.

In this section we will go over a summary of general caracteristics we observed about phishing pages, followed by a more detailed analysis of the browser APIs and adversarial techniques we observed. We will then discuss how we used browser APIs to cluster phishing kits, and how we were able to identify the existence of a common kit among different pages.

\subsection{Overview}

\paperFinding{Majority of phishing pages execute javascript code that uses a browser API or property} \totalURLsWithBrowserAPIs{} out of \totalURLs{} (\totalURLsWithBrowserAPIsPrecent{}) urls execute javascript code that uses a browser API.Out of these urls, we focus our analysis on \totalURLsWithBrowserAPIsFirstPartyRoot{} urls that execute javascript code from a first-party relative to the URL we got from the  (\totalURLsWithBrowserAPIsFirstPartyRootPrecent{}) to reduce likelyhood that we are looking at a domain that has cloacked away. An average page executes 24 browser APIs ($\sigma=46$, median=5), 7 property writes ($\sigma=12$, median=2), and 92 property reads ($\sigma=237$, median=10).

\paperFinding{Phishing pages very wildly from the brand that they are mimicing}

\paperFinding{Phishing pages employ experimental browser APIs} MDN marks \MDNExperimentalTotal{} browser APIs, out of those we see \numberofExperimentalAPIs{} in our dataset at least once. We find that the top 3 experimental APIs are \textit{NavigatorUAData.getHighEntropyValues()}, \textit{Keyboard.lock()}, and \textit{Scheduling.isInputPending()}  (8,163/3,830/3,749 pages respecfully). \textit{NavigatorUAData.getHighEntropyValues()} is used for fingerprinting, while \textit{Keyboard.lock()} captures the keys passed as an argument, even if a modifier key is pressed. However, this API requires a user interaction (like pressing on an element) in order to call. \textit{Scheduling.isInputPending()} is used to check if there is an input event pending, and is used to determine if the page is being interacted with. 

% In addition to this, we find that 5,281 pages use WebAssembly. For the pages that invoke WebAssembly.compile since the module is passed down via base64, we pull 33 WASM distinct modules (from both first party and third party scripts) and establish 23 are used for bot detection, and 3 for captcha .



\paperFinding{Phishing pages make network requests using javascript before any user interaction} We check how common are data exfiltration related APIs in our dataset. We report a full breakdown of the categories in Table~\ref{tab:exfil_calls}. In the top 3 are \textit{XMLHttpRequest.send} and \textit{Window.fetch}, viewing the arguments passed to these functions through the VisibleV8 Logs, we see that 

\input{figures/exfil_calls.tex}

\paperFinding{Changepoints in browser API usage map to introduction of newly bundled packages or new techniques} Using the PELT and the parameters described in Section~\ref{sec:methods} we are able to detect XXX apis that exhibit changepoints. What does this mean?

\subsection{Browser APIs and Adversarial Techniques}
\paperFinding{We see a mixture of obfuscation techniques in our dataset} We obse

\paperFinding{Very few (992 out of 800,000) pages request pop-up permission upon the page loading} This is a smaller fraction of pages then in in crawlphish. This, could be an result of Firefox, citing low engagement with the notifications, started requireing a user interaction to trigger the popup\todocite{Mozila release} at the end of November 2019, when crawlphish's data collection ends. Lack of pop-up requests could also be explained by our Finding-XX reguarding usage of Fetch and XMLHttpRequest or by the overwhelming amount of the pages (67\%) registering at least one HTMLElement event handler, which would be classified as a \textit{Click-through} by Crawlphish's taxonomy. We report a full breakdown of APIs related to the crawlphish categorization of client-side cloaking in Table~\ref{tab:crawlphish}.

We also see a lower usage of \textit{Navigator.userAgent}, the javascript property for reading the user-agent. This signals that phishing pages are follwing along with the recomendation to not use this property, as the specification states that a client should give very little about the machine via this property\todocite{https://developer.mozilla.org/en-US/docs/Web/API/Navigator/userAgent}. Nowever browsers have moved to revealing less and less about the user through the userAgent, for example, latest build of Safari on macOS 16.2 will have show macOS 10.15.7\todocite{\url{https://bugs.webkit.org/show_bug.cgi?id=276718}}.
\input{figures/crawlphish_all_categories.tex}

\paperFinding{Out of 846,397 pages, 650,782 execute at least one of the APIs, 244,591 execute at least 5 and 128,229 execute at least 10} We find that

\subsection{Browser APIs and Phishing Kits}
\paperFinding{Browser API sets help identify the existence of a common kit among different pages} For the XXX pages from XXX kits, we compute a XXXX by XXX matrix of distances using Jaccard index. The FMI (harmonic mean of TP and TN) comes to \textbf{0.92} with the vmeasure of \textbf{0.87}. Using a higher $\beta$ value for the V-measure we can see that the model is more complete then homogeneous, meaning that it has a tendency of merging clusters together. Should be noted that HDBSCAN gave us simular results as DBSCAN with a relativly low epsilon value, though for a data source with larger quantities of data, we would recomend limiting the epislon manually, setting it as a max alike ratio. In eight different cases, HDBSCAN clustered together pages with 0 simularity. 

\paperFinding{Filtering out different types of APIs gives negligable returns, while lowering how many pages you can try to cluster} We experiment with two setups, removing all DOM related APIs, and removing all getters from the sets. There are XXX kits that have pages executing enough APIs to meet our criteria for clustering.

\paperFinding{Clusters of pages via API simularity display either very burtsy or drawn out campaigns} Clustering temporial clusters and merging them using the methods described in Section-~\ref{sec:methods}, we see clusters well formed clusters with the average silhouette score of 0.74 ($\sigma=0.06$, median=0.73). Merging those clusters together, based on the requirement of the new merged cluster having at least 8 browser APIs in common between all pages, and share at least 4 pages across cluster, gives us a total of 15,401 clusters, from a total of 473,704 pages. 
These clusters on average go on for 30 days ($\sigma=20$) with the longest one spanning 257 days, with 237 pages. Manually inspecting the cluster reveals these to be microsoft phishing pages with an average pHash simularity of \textbf{XXX}.