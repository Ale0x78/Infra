\section{Results (4-5 pages)} 
\label{sec:results}

Crawling the phishing pages for a total of \totalDaysCrawled{} days, we process a little over \totalURLs{} distinct urls (from \totalDomains{} distinct domains). 
Meanwhile, kitphishr identified \numberKits{} zip files which we de-duplicate into \numberKitFamilies{} kits from \numberofGTDomains{} with an average of \numberofGTDomainperKit{} domains per kit ($\sigma{}=\numberofGTDomainperKitSTD{}$ and a max of 42) Since we require at least two domains from a kit to be able to cluster it. For the domain to execute at least four browser APIs and property accesses, we further filter the kits to \totalKitsToCluster{} kits.

In this section, we will summarize the general characteristics we observed about phishing pages, followed by a more detailed analysis of the browser APIs and adversarial techniques we observed. We will then discuss how we used browser APIs to cluster phishing kits and how we identified the existence of a common kit among different pages.

\subsection{Overview}

\paperFinding{Majority of phishing pages execute javascript code that uses a browser API or property} \totalURLsWithBrowserAPIs{} out of \totalURLs{} (\totalURLsWithBrowserAPIsPrecent{}) urls execute javascript code that uses a browser API.Out of these urls, we focus our analysis on \totalURLsWithBrowserAPIsFirstPartyRoot{} urls that execute javascript code from a first-party relative to the URL we got from the  (\totalURLsWithBrowserAPIsFirstPartyRootPrecent{}) to reduce the likelihood that we are looking at a domain that has cloaked away. An average page executes 24 browser APIs ($\sigma=46$, median=5), 7 property writes ($\sigma=12$, median=2), and 92 property reads ($\sigma=237$, median=10).

\paperFinding{Phishing pages vary wildly from the brand that they are mimicing}

\paperFinding{Phishing pages employ experimental browser APIs that are not implemented by every version of the major browsers} MDN marks \MDNExperimentalTotal{} browser APIs, out of those we see \numberofExperimentalAPIs{} in our dataset at least once. We find that the top 3 experimental APIs are \textit{NavigatorUAData.getHighEntropyValues()}, \textit{Keyboard.lock()}, and \textit{Scheduling.isInputPending()}  (\textbf{8,163/3,830/3,749} pages respecfully). \textit{NavigatorUAData.getHighEntropyValues()} is used for fingerprinting, while \textit{Keyboard.lock()} captures the keys passed as an argument, even if a modifier key is pressed. However, this API requires a user interaction (like pressing an element) to call. \textit{Scheduling.isInputPending()} is used to check if an input event is pending and to determine if the page is being interacted with. 

% In addition to this, we find that 5,281 pages use WebAssembly. For the pages that invoke WebAssembly.compile since the module is passed down via base64, we pull 33 WASM distinct modules (from both first party and third party scripts) and establish 23 are used for bot detection, and 3 for captcha .

\input{figures/ipinfo}

\input{figures/exfil_calls.tex}

\paperFinding{Phishing pages make network requests using javascript before any user interaction} We check how common data exfiltration-related is in our dataset. We report a full breakdown of the categories in Table~\ref{tab:exfil_calls}. In the top 3 are \textit{XMLHttpRequest.send} and \textit{Window.fetch}, viewing the arguments passed to these functions through the VisibleV8 Logs, we see that out of 220,377 requests were made via JavaScript.


\paperFinding{Changepoints in browser API usage map to the introduction of newly bundled packages or new techniques} Using the PELT and the parameters described in Section~\ref{sec:methods}, we can detect 405 APIs (288 of which are not DOM related) have a timeseries that can be segmented into timeslices based on divination from the mean.
\subsection{Browser APIs and Adversarial Techniques}

\paperFinding{We see a mixture of obfuscation techniques in our dataset} Dispite best recomendations\cite{EvalJavaScriptMDN2025} to webdevelopers, we see scripts execute multiple javascript eval statements. Sometimes, a script that are being executed via `eval()' evaluate yet another script itself, we measure this phenomena as a level in \textbf{eval-depth}. Over 22,000 pages hide javascript evaluation behind a single eval statement, but 744 pages have chains of evals. While we find 12,781 pages requesting a javascript object used for AES encryption, we only see 1,224 pages that call decrypt during the visit. We see 12.5\% of pages invoke a base64 or URL decoding API, frequently associated with obfuscation \cite{jsufp}. We also identify 125 scripts that are identifical based on their script hash, but are present at different eval depths on different URLs.

\paperFinding{Very few (992 out of 800,000) pages request pop-up permission upon the page loading} This is a smaller fraction of pages than in Crawlphish. This could be a result of Firefox, citing low engagement with the notifications, started requiring user interaction to trigger the popup\cite{mozillaRestrictingNotificationPermission2019} at the end of November 2019, when crawlphish's data collection ended. Chrome has since discussed making modifications to the notification API to make the request less disruptive to the user-experiance\cite{IntroducingQuieterPermission}. The lack of pop-up requests could also be explained by our \textbf{Finding-XX} regarding usage of Fetch and XMLHttpRequest or by the overwhelming amount of the pages (67\%) registering at least one HTMLElement event handler, which would be classified as a \textit{Click-through} by Crawlphish's taxonomy. We report a full breakdown of APIs related to the crawlphish categorization of client-side cloaking in Table~\ref{tab:crawlphish}.

We also see a lower usage of \textit{Navigator.userAgent}, the javascript property for reading the user-agent. This signals that phishing pages are following along with the recommendation to not use this property, as the specification states that a client should give very little about the machine via this property\cite{NavigatorUserAgentProperty2024}. Nowever browsers have moved to revealing less and less about the user through the userAgent, for example, latest build of Safari on macOS 16.2 will have show macOS 10.15.7\cite{276718StopConditionally}.
\input{figures/crawlphish_all_categories.tex}

\paperFinding{Out of 846,397 pages, 650,782 execute at least one of the APIs, 244,591 execute at least 5, and 128,229 execute at least 10} These numbers change to 283,644, 166,958 and 96,283 if you change the requirement to include at least one data exfiltration api.

\subsection{Browser APIs and Phishing Kits}
\paperFinding{Browser API sets help to identify the existence of a standard kit among different pages} For the 2,461 pages from 162 kits, we compute an 2,461 by 2,461 matrix of distances using the Jaccard index. The FMI (harmonic mean of TP and TN) comes to \textbf{0.97} with the v-measure of \textbf{0.81} and a silhouette score of \textbf{0.87}. Using a higher $\beta$ value for the V-measure, we can see that the model is more complete than homogeneous, meaning it tends to merge clusters. It should be noted that HDBSCAN gave us similar results as DBSCAN with a relatively low epsilon value, though, for a data source with larger quantities of data, we would recommend limiting the epsilon manually, setting it as a max similar to the ratio. In eight different cases, HDBSCAN clustered together pages with zero similarity. 

HDBSCAN allows for single kit clusters; given some points are distant enough from everything else, the algorithm can isolate some single points as solo clusters. Clustering all pages that we have kits for, even if they do not meet the requirement of having at least 2 page, with support for single clusters enabled, we see an FMI of 0.92, with a V-measure of 0.79.

\paperFinding{Filtering out different types of APIs gives negligible returns while lowering how many pages you can try to cluster} We experiment with two setups, removing all DOM-related APIs and removing all getters from the sets. There are XXX kits that have pages executing enough APIs to meet our criteria for clustering.

\paperFinding{Clusters of pages via API similarity display either very bursty or drawn out campaigns} Clustering temporal clusters and merging them using the methods described in Section-~\ref{sec:methods}, we see clusters well-formed clusters with the average silhouette score of 0.74 ($\sigma=0.06$, median=0.73). Merging those clusters, based on the requirement of the new merged cluster having at least eight browser APIs in common between all pages and sharing at least four pages across the cluster, gives us a total of 15,401 clusters, from a total of 473,704 pages. 
These clusters, on average, go on for 30 days ($\sigma=20$), with the longest one spanning 257 days, with 237 pages. Manually inspecting the cluster reveals these to be Microsoft phishing pages with an average pHash similarity of \textbf{XXX}.