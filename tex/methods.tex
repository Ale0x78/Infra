\section{Methodology}
\label{sec:methods}
\input{figures/infra}
 This paper provides a methodology for reducing the space of phishing pages we observe over the \daysCrawled{} days, evaluates how closely the clusters resemble underlying phishing kits, and describes how widespread different adversarial techniques are in the ecosystem. We require browser API execution traces from phishing pages and a subset of pages where we know the underlying phishing kit for this. In the following section, we describe the experimental setup for gathering this data, the steps we took to aggregate and enrich the execution traces, how we identified anomalies and trends, and finally, the steps we took for clustering the data and evaluating them as an analog for phishing kits. A full overview of our crawling and analysis pipeline can be seen in Figure-\ref{fig:infra}.


\subsection{Data Gathering}

Our crawling infrastructure aims to ingest phishing URLs from upstream providers and output execution traces from the page, as well as a potential kit used for that page.

\myparagraph{URL feeds} We gathered phishing pages from a diverse set of phishing feeds, monitoring OpenPhish~\cite{openphish}, PhishTank~\cite{phishtank}, URLScan~\cite{urlscan}, SMS Gateways~\cite{nahapetyan2024sms}, PhishDB~\cite{phishingDB}, and APWG~\cite{apwg}, based on the availability of the feed and the level of access we had at the time. Every hour, we checked these feeds for new urls (limited to the last 48 hours) and submitted them into two different crawlers: VisibleV8 and KitPhished.

\myparagraph{VisibleV8} To get execution traces for every script loaded when visiting the page, we used an automated chromium-based crawler with VisibleV8 patches applied. The browser is being automated to visit the page and take screenshots with puppeteer~\cite{pptr}, an NPM package by Google to help in UI/UX testing and browser automation. The patched chromium crawler uses puppeteer-stealth, a set of configurations to help mask the headless chrome and puppeteer itself from detection tools~\cite{puppeteerstealth}. We initiated the crawls from a network designated for research purposes, for which the ISP would register as `educational' for any IP intelligence API and used Catapult~\cite{catapult}, a man-in-the-middle proxy, to capture the entire HTTP archive for replayability. The crawler stays on the page for 45 seconds before taking a screenshot to allow scripts to load and start executing; this is consistent with prior work~\cite{vv8-imc19,englehardt2016census}.

\myparagraph{KitPhisher} While not guaranteed, some malicious actors leave the zip files of the kits used in a discoverable folder on the same server that hosts the website (for example, the Apache document root). KitPhisher~\cite{kitphishr} is a Go-based URL fuzzer that attempts to identify any leftover zip files on the server. Prior work~\cite{liu_inferring_2022,oest_inside_2018} establishes a method to collect and analyze phishing kits. If successful, it will download the zip file and make a note of the domain from which kitphishr acquired it.

\subsection{De-duplication}

Once we have collected browser API traces and potential phishing kits, we will post-process the traces into a set of APIs executed in a 1st-party context per page and deduplicate the phishing kits based on archive file similarity.

\subsubsection{Trace postprocessing} VisibleV8 has a default log-postprocessor set. These programs take the raw lags generated by the patched Chromium browser and convert it into an organized database, identifying duplicate scripts via sha3 hash, clearly marking the origin of each script that loaded and isolating which JavaScript API calls are browser API calls defined in the WebIDL file\footnote{\url{https://developer.mozilla.org/en-US/docs/Glossary/WebIDL}} generated during the while building the patched chromium. For our analysis, we use a tuple of the original page's URL, the script's URL, and an unordered set of APIs executed by this script. 

To not introduce artifacts into our clusters from cloaked pages and 3rd-party scripts like Google Analytics, known to be present in phishing pages~\cite{phishingvLegit}, we isolate API sets executed by 1st-party scripts (from now on called 1st-party API sets). We establish the root domain as the domain submitted to the feeds and the origin as the domain from which the script is loaded. We consider a script 1st-party only if it is loaded from the domain we acquired from our feeds (root domain). The only exception is when we identify which pages embed a Cloudflare Turnstile script. As some of the Turnstile scripts can be hosted on `Cloudflare.com.'


\myparagraph{De-duplicating kit files} We crawl the URLs with KitPhisher to establish a ground truth dataset with URLs originating from the same kit. For zip files extracted via KitPhisher that have password-based encryption enabled, we use the SHA256 hash of the zip files to identify which domains yielded the same kit. For the remaining zip files, we further de-duplicate them into Kit-Families by looking at them as a set of SHA256 of source files\footnote{Source files identified via a python libmagic module~\cite{libmagic}}. If the Jaccard index-based similarity ($JI(A,B) = |A \cap B| / |A \cup B|$) of these sets is equal to or greater than 90\%, we consider the two kits to be from the same family, and thus group all domains from both kits into the same group.

\myparagraph{Adjusting for Cloudflare} We find a high number of anomalies originate from Cloudflare scripts on the same domain as the phishing pages. Since Cloudflare scripts load from a URL with a \textbf{`cdn-cgi'} in the URL, for most of our analysis, we do not consider scripts loaded from that endpoint. However, we discuss the behaviors we could see from these scripts in Section-\ref{sec:results}.

\subsection{Data enrichment}

In addition to data from our phishing feeds, we compile API usage from brand's original pages in order to discuss the differences between phishing pages and the login pages they are targetting in Section-\ref{sec:discussion} and we present a mapping of different adversarial technique employed by phishing pages mapped to different API usages.

\myparagraph{Technique to Browser API mapping} Client-side JavaScript can engage in data harvesting (exfiltrating information dynamically and not through form submission), evasion (conditional dynamic behavior aimed at hiding functionality or contents), obfuscation (unconditional behavior meant to hinder static analysis), and mimicking (dynamic behavior to make the page more believable, for example, false loading pages, stage by stage data extraction). Based on prior work by Su~\etal{} and Zhang~\etal{} and manually identifying APIs from the Mozila Developers Network (MDN) documentation, we present a table mapping standard phishing techniques to browser APIs in Table-\ref{tab:behaviorcategories}.We leverage the presence of these APIs in the execution traces as a signal of the technique being present in the page. If the page falls within a certain cluster, we mark the entire cluster as using that technique. This is done because we observe phishing pages load scripts that engage in non-deterministic behavior, as well as the difference in the load on our infrastructure, can cause different amounts of work (lines of code) to be executed between different visits. For Cloudflare turnstiles embedding, we use a non-browser API as our detection metric as Cloudflare's native turnstile script will read the value of \texttt{Window.Turnstiles}.
For Client-side IP checks, we manually looked through every \texttt{Window.fetch} and \texttt{XMLHttpRequest.open} argument URL that were present in more than 50 pages, and manually identified 15 that were IP reputation APIs. 
\input{figures/behavior_categories}

\myparagraph{Brand's Original page} OpenPhish and APWG's eCrime Exchange report the brand that a phishing page targets. We selected 10 out of the top 52 brands targeted and collected VisibleV8 logs for their home pages and, when applicable, their login pages, to assess how similar phishing pages are to their target pages. Lin~\etal{} in ~\cite{lin_phish_2022} identified that phishing pages could effectively use browser fingerprints to bypass multi-factor authentication, and ~\cite{rola_rods_2023} found that phishing pages deploy a plethora of 3rd party fingerprinting scripts, which differ from the scripts of the original page, what is the simularity between browser API usage between the target brand's page, and the phishing counterpart. 

\subsection{Identifying kits}

We hypothesize that similarity in browser API execution means that the pages originate from the same phishing kit. In order to test this, we ingest API sets from pages we were able to identify phishing kits for, and output a potential clustering of those pages, checking how well the clustering maps back to the ground truth information.

To establish this similarity, we use the Jaccard index on the API sets that 1st-party scripts execute. We use Hierarchical Density-Based Spatial Clustering of Applications with Noise~\cite{hdbscan} (HDBSCAN), with a minimum cluster size of 2, to cluster the pages with the Jaccard distance as our distance kernel. With HDBSCAN requiring minimal fine-tuning out of the box, and requiring no prior knowledge of the number of clusters we need to look for, we use the ground truth labels from KitPhisher to evaluate the clustering. When ground truth is available, we evaluate the clustering using the \textbf{ Fowlkes-Mallows Index}~\cite{fowlkes1983method} and \textbf{ V-measure}. 
\textbf{V-measure} is a harmonic mean between completeness (all members in a cluster are from the right class) and homogeneity (all members in a cluster are from the same class). V-measure allows tuning a ratio $\beta$ that prioritizes the score towards one vs the other. More importantly, looking at completeness and homogeneity scores lets you see how your clustering approach is getting things wrong~\cite{Rosenberg2007VMeasureAC}.
We use sklearn's measure module to calculate all cluster evaluation metrics.~\cite{scikit-learn}. For the much larger set of pages for which we do not have kit labels, we use \textbf{silhouette score}\footnote{While silhouette score is biased against non-convex clusters, based on our results, we do not see it necessary to switch to a density-specific cluster metric}, a metric for how well packed and separated the clusters are, to evaluate the clustering when we do not have ground truth. 

In order to process \totalPagesWithJavascriptFP{} pages in one go, we divide the pages into small time windows and first cluster them into smaller local clusters. Figure-\ref{fig:infra} shows the breakdown of our methodology. But we first divide our data using a 3-week rolling window; we roll by 2 weeks. However, pages now may exist across two clusters, so once we cluster these chunks using HDBSCAN, with no hyperparameter tuning and a minimum cluster size of 2, we merge any 2 clusters with at least one page in common across the sliding window. We refer to these clusters as `local clusters'. However, these clusters can not represent phenomena like the re-emergence of clusters, if it happens after 2 weeks. To resolve this, we use the set intersection of APIs (representative API set for the clusters) from every page within a local cluster. We use DBSCAN with a conservative $\epsilon=0.05$ to merge them. Intuitively, this $\epsilon$ represents the maximum dissimilarity you are willing to tolerate between clusters. We selected this $\epsilon$ value by experimenting on the clusters of pages from the ground truth set. When it comes to merging local clusters, $\epsilon=0.04$ yields fewer clusters and a better silhouette score; however, to avoid data-peeking, as we use the silhouette score to describe the shape of the final clusters, we chose not to determine the value based on the final clusters. We present the clustering metrics from both local clusters and final clusters in Section-\ref{sec:results}.
To avoid calculating a distance matrix on the over four hundred thousand pages successfully clustered, we use the representative set from each local cluster to compute the silhouette score on the final clusters. We separate local clusters labeled as noise in the 2nd cluster step (with DBSCAN and $\epsilon=0.05$) into single clusters. 

\subsection{Identifying trends}

In order to discuss the differences between a singular API changes over time, and a clustering of pages via their API sets, we attempt to identify temporal anomalies in every API's time series. We also discuss here how we chose to quantify the lifetime of clusters and the deployment diversity of different clusters.

\myparagraph{Identifying anomalies} We look at the overall anomalous patterns for a timeseries of every browser API we observe. We use rapture~\cite{ruptures} to detect change points in the time series of API calls. We use a linearly penalized segmentation algorithm (PELT), which combines a penalty parameter $\beta$ and a cost function to quickly partition a time series based on points where it detects the series starts to change. For the cost function, we use \textbf{L1Cost}, which uses a deviation in the mean, and for the penalty parameter, we use $\beta=2 \ln{(n)=12}$ where $n$ is the number of days in the timeseries. This aligns\todowrite{Sivana says, re-phrase this, but also this entire paragraph needs a re-work} with the recommendation of Killick~\etal{} in the paper introducing PELT to minimize the Bayesian information criterion. We normalize by our daily URLs ingested and look at a time series of the percentage of daily URLs that executed the given URL. To clear up the results from rapture, we look at the change in the mean between segments. If we are less than 3\%, we ignore and merge with the prior segment.

\myparagraph{Cluster lifetime and deployment diversity} Throughout this work, we refer to cluster lifetime as the time range between when the first page belonging to the cluster is observed on the feed and when the last page belonging to the cluster is observed on the feed. We pull and crawl URLs from the phishing feeds every two hours, meaning that this is an approximation of when the URLs appear on the feeds, with an error of two hours. 
We measure deployment diversity of the phishing pages by looking at the effective 2nd-level domain (e2LD) for the URLs. Using the e2LD instead of the entire hostname ensures that pages deployed on 'pages.dev` or 'blogger.com` are considered a single deployment form. 