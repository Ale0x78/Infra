\section{Methodology}
\label{sec:methods}
\input{figures/infra}
 This paper provides a methodology for clustering using dynamic features, evaluating how closely the clusters resemble underlying phishing kits, and describing how widespread different adversarial techniques are in the ecosystem. The building blocks of our experimental design are browser API execution traces from phishing pages and a ground-truth dataset of pages where we know the underlying phishing kit. In the following section, we describe the experimental setup for gathering this data, the steps we took to aggregate and enrich the execution traces, annotate the clusters based on different techniques, and finally, the steps we took for clustering the data and evaluating them as analogs for phishing kits. An overview of our crawling and analysis pipeline can be seen in Figure~\ref{fig:infra}.
 
\subsection{Data Gathering}

Our crawling infrastructure aims to ingest phishing URLs from upstream providers and output execution traces from the page, as well as a potential kit used for that page.

\myparagraph{URL feeds} We gathered phishing pages by monitoring the following phishing feeds: OpenPhish~\cite{openphish}, PhishTank~\cite{phishtank}, URLScan~\cite{urlscan}, SMS Gateways~\cite{nahapetyan2024sms}, PhishDB~\cite{phishingDB}, and APWG~\cite{apwg}, based on the availability of the feed and the level of access we had at the time. Every two hours, we checked these feeds for new URLs (limited to the last 48 hours) and submitted them into two different crawlers: VisibleV8 and \kp{}.

\myparagraph{VisibleV8} To get execution traces for every script loaded when visiting the page, we used an automated Chromium-based crawler with VisibleV8 patches applied~\cite{vv8-imc19}. The VisibleV8 patches modify Chromium to output a log of all JavaScript APIs executed for every script loaded on the page. We automate the browser to visit the page and take screenshots with puppeteer~\cite{pptr}, an NPM package by Google to help in UI/UX testing and browser automation. The patched Chromium crawler uses puppeteer-stealth, a set of configurations to help mask the headless Chrome and Puppeteer itself from detection tools~\cite{puppeteerstealth}. We initiated the crawls from a network designated for research purposes, for which the ISP would register as `educational' for any IP intelligence API. We used Catapult~\cite{catapult}, a man-in-the-middle proxy, to capture the entire HTTP archive for replayability. The crawler stays on the page for 45 seconds before taking a screenshot to allow scripts to load and start executing, consistent with prior work~\cite{vv8-imc19,englehardt2016census}.

\myparagraph{\kp{}} While not guaranteed, some malicious actors leave the zip files of the kits used in a discoverable folder on the same server that hosts the website (for example, the Apache document root). \kp{}~\cite{kitphishr} is a Go-based URL fuzzer that attempts to identify any leftover zip files on the server. Prior work~\cite{liu_inferring_2022,oest_inside_2018} establishes \kp{} as a method for collecting and analyzing phishing kits. If successful, it will download the zip file and make a note of the domain from which \kp{} acquired it.

\subsection{De-duplication}

Once we have collected browser API traces and potential phishing kits, we post-process the traces into a set of APIs executed in a 1st-party context per page and de-duplicate the phishing kits.

\subsubsection{Trace postprocessing} VisibleV8 has a default log-postprocessor set. These programs take the raw lags generated by the patched Chromium browser and convert it into an organized database, identifying duplicate scripts via SHA3 hash, clearly marking the origin of each script that loaded and isolating which JavaScript API calls are browser API calls defined in the WebIDL file\footnote{\url{https://developer.mozilla.org/en-US/docs/Glossary/WebIDL}} generated while building the patched Chromium. For our analysis, we extracted tuples of the original page's URL, the script's URL, and an unordered set of APIs executed by this script.

To not introduce artifacts into our clusters from cloaked pages and 3rd-party scripts like Google Analytics, known to be present in phishing pages~\cite{phishingvLegit}, we isolate API sets executed by 1st-party scripts (from now on called 1st-party API sets). We establish the root domain as the domain submitted to the feeds and the origin as the domain from which the script is loaded. We consider a script 1st-party only if it is hosted on the same domain we acquired from our feeds (root domain). The only exception is when we identify which pages embed a Cloudflare Turnstile script. As some of the Turnstile scripts are hosted on `Cloudflare.com.' 


\myparagraph{Identifying Kit-families} We crawl the URLs with \kp{} to establish a ground truth dataset with URLs originating from the same kit\footnote{For any domain that yielded two or more zipfiles, we discard all}. For zip files extracted via \kp{} that have password-based encryption enabled, we use the SHA256 hash of the zip files to identify which domains yielded the same kit. For the remaining zip files, we further de-duplicate them into Kit-Families, examining them as a set of SHA256 hashes of source files\footnote{Source files identified via a python Magika module~\cite{fratantonio25:magika}}. If the Jaccard index-based similarity, where $JI(A, B) = |A \cap B| / |A \cup B|$, of these sets is equal to or greater than 90\%, we consider the two kits to be from the same family, and thus group all domains from both kits into the same group.

\myparagraph{Adjusting for Cloudflare} Many anomalies (multi-layer evals, non-deterministic behavior, dynamically updated scripts) originate from Cloudflare scripts on the same domain as the phishing pages. Since Cloudflare scripts load from a URL with a \emph{`cdn-cgi'} in the path, for most of our analysis, we do not consider scripts loaded from that endpoint.

\subsection{Identifying kits}

We hypothesize that similarity in browser API execution means that the pages originate from the same phishing kit. To test this, we ingest API sets from pages for which we were able to identify phishing kits and output a potential clustering of those pages, checking how well the clustering maps back to the ground truth information.

To establish this similarity, we use the Jaccard index on the 1st-party API sets with Hierarchical Density-Based Spatial Clustering of Applications with Noise~\cite{hdbscan} (HDBSCAN), with a minimum cluster size of 2, to cluster the pages with the Jaccard distance as our distance kernel. With HDBSCAN requiring minimal fine-tuning out of the box, and requiring no prior knowledge of the number of clusters we need to look for, we use the ground truth labels from \kp{} to evaluate the clustering. When ground truth is available, we evaluate the clustering using the \emph{Fowlkes-Mallows Index}~\cite{fowlkes1983method} and \emph{V-measure}. We use sklearn's measure module to calculate all cluster evaluation metrics~\cite{scikit-learn}, and separate all the noise elements into singleton clusters for evaluation, as removing all noise samples would give it an unreasonable advantage; however, keeping the elements in the same noise cluster would artificially reduce the homogeneity score. For the unlabeled set of pages for which we do not have kit labels, we use silhouette score\footnote{While silhouette score is biased against non-convex clusters, based on our results, we do not see it necessary to switch to a density-specific cluster metric}, a metric for how well packed and separated the clusters are, to evaluate the clustering when we do not have ground truth.

To process \totalPagesWithJavascriptFP{} pages would require a distance matrix of distances (64-bit float) over a 1.5Tb in size. To resolve this restriction, we first divide our data using a 4-week rolling window, rolled by 2 weeks, and cluster pages within that window. We refer to these clusters as `local clusters'. However, these local clusters can not represent phenomena like the re-emergence of kits if it happens after 2 weeks and contains duplicates of the same page. To resolve this, we merge any two clusters with at least 1 page in common between local clusters and merge them using the representative API set for the clusters (API set intersection of every page in the cluster). We use DBSCAN with a conservative $\epsilon=\dbscanEpsi$ to merge them. Intuitively, this $\epsilon$ represents the maximum dissimilarity tolerated between clusters, we pick \dbscanEpsi to maximise the silhouette score of the final clusters.

\subsection{Data enrichment}
\input{figures/behavior_categories}
In addition to all the data gathered, our analysis references a manually crafted Browser API to phishing technique mapping, JavaScript execution traces for the top 5 brands' login pages targeted in our dataset, and characterization of cluster lifetimes and deployment diversity.

\myparagraph{Technique to Browser API mapping} Client-side JavaScript can engage in data harvesting (exfiltrating information dynamically and not through form submission), evasion (conditional dynamic behavior aimed at hiding functionality or contents), obfuscation (unconditional behavior meant to hinder static analysis), and mimicking (dynamic behavior to make the page more believable, for example, false loading pages, stage by stage data extraction). Based on prior work by Su~\etal{} and Zhang~\etal{} and manually identifying APIs from the Mozilla Developers Network (MDN) documentation, we present a table mapping standard phishing techniques to browser APIs in Table~\ref{tab:behaviorcategories}.We leverage the presence of these APIs in the execution traces as a signal of the technique being present in the page. If the page falls within a specific cluster, we mark the entire cluster as using that technique. We use this to combat the phishing pages that engage in non-deterministic behavior. For Cloudflare turnstiles embedding, we use a non-browser API as our detection metric, as Cloudflare's native turnstile script will read the value of \texttt{Window.Turnstiles}.
For Client-side IP checks, we manually looked through every \texttt{Window.fetch} and \texttt{XMLHttpRequest.open} argument URL given that argument was present in more than 50 phishing pages, and identified 15 that were IP reputation APIs. 


\myparagraph{Brand's Original page} OpenPhish and APWG's eCrime Exchange report the brand that a phishing page targets. We selected 5 out of the top 52 brands targeted based on popularity by page number and brands that represented seasonality targeted sectors (like the IRS or banks). We collected VisibleV8 logs for their home pages and, when applicable, their login pages, to assess how similar phishing pages are to their target pages. 

\myparagraph{Cluster lifetime and deployment diversity} Throughout this work, we refer to cluster lifetime as the time range between when we observe the first page belonging to the cluster on a phishing feed and when we observe the last page belonging to the cluster. 
We measure deployment diversity of the phishing pages by looking at the effective 2nd-level domain (e2LD) for the URLs. Using the e2LD instead of the entire hostname ensures that pages deployed on 'pages.dev` or 'blogger.com` are considered a single deployment form. 