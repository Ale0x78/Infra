\section{Methodology (1 page)}
\label{sec:methods}
\input{figures/infra}
This paper aims to study the evolution of browser API usage in phishing pages and leverage execution traces of these pages to identify which ones share an underlying kit. To accomplish this, we require browser API execution trances from phishing pages over a long period and a subset of pages where we know the underlying phishing kit. In the following section, we describe the experimental setup for gathering this data, the steps we took to aggregate and enrich the execution traces, how we identified anomalies and trends, and finally, the steps we took for clustering the data and evaluating them as an analog for phishing kits. A full overview of our crawling and analysis pipeline can be seen in \ref{fig:infra}.


\subsection{Data Gathering}
\myparagraph{URL feeds} We gathered phishing pages from a diverse set of phishing feeds, monitoring OpenPhish\cite{openphish}, PhishTank\cite{phishtank}, URLScan\cite{urlscan}, SMS Gateways\cite{nahapetyan2024sms}, PhishDB\cite{phishingDB}, and APWG\cite{apwg}, based on the availability of the feed and the level access we had at the time. Every hour, we checked these feeds for new urls (limited to the last 48 hours) and submitted them into two different crawlers: VisibleV8 and KitPhished.

\myparagraph{VisibleV8} To get execution traces for every script loaded when visiting the page, we used an automated chromium-based crawler with VisibleV8 patches applied. The browser is being automated to visit the page and take screenshots with puppeteer\cite{pptr}, an NPM package by Google to help in UI/UX testing and browser automation. The patched chromium crawler uses puppeteer-stealth, a set of configurations to help mask the headless chrome and puppeteer itself from detection tools\cite{puppeteerstealth}. We initiated the crawls from a university network designated for research purposes and used Catapult\cite{catapult}, a man-in-the-middle proxy, to capture the entire HTTP archive for replayability. The crawler stays on the page for 45 seconds before taking a screenshot to allow scripts to load and start executing; this is consistent with prior work\cite{vv8-imc19,englehardt2016census}.

\myparagraph{KitPhisher} While not guaranteed, some malicious actors leave the zip files of the kits used in a discoverable folder on the same server that hosts the website (for example, the Apache document root). KitPhisher\cite{kitphishr} is a go-based URL fuzzer that attempts to identify any left-over zip files on the server. Prior work\cite{liu_inferring_2022,oest_inside_2018} establishes it as an excellent way to collect and analyze phishing kits. If it is successful, it will download the zip file and make a note of the domain from which kitphishr acquired it.

\myparagraph{Brand's Original page} OpenPhish and APWG's eCrime Exchange report the brand that a phishing page targets. We selected 10 out of the top 100 brands targeted and collected VisisbleV8 logs for their home pages and, when applicable, their login pages. This is to assess how similar phishing pages are to their target pages. Lin \etal{} in \cite{lin_phish_2022} identified that phishing pages could effectively use browser fingerprints to bypass multi-factor authentication, and \cite{rola_rods_2023} found that phishing pages deploy a plethora of 3rd party fingerprinting scripts, which differ from the scripts of the original page, what is the simularity between browser API usage between the target brand's page, and the phishing counterpart. 

\subsection{Data enrichment and De-duplication}
\subsubsection{Enriching browser API executions}

\myparagraph{Browser APIs and property accesses} VisibleV8 has a default log-postprocessor set. These programs take the raw lags generated by the patched Chromium browser and convert it into an organized database, identifying duplicate scripts via sha3 hash, clearly marking the origin of each script that loaded and isolating which javascript API calls are browser API calls defined in the WebIDL file\footnote{\url{https://developer.mozilla.org/en-US/docs/Glossary/WebIDL}} generated during the while building the patched chromium. For our analysis, we use a tuple of the original page's URL, the script's URL, and a set (unordered) of APIs executed by this script. 


\myparagraph{First party API sets} To cut through the noise of cloaked pages and include third-party scripts like Google Analytics, known to be present in phishing pages\cite{phishingvLegit}, we isolate API sets executed by first-party scripts (from now on called first-party API sets). To do this, we establish the root domain as the domain submitted to the feeds and the origin as the domain from which the script is loaded. We consider a script first-party only if loaded from the domain we acquired from our feeds (root domain). 

\myparagraph{Browser APIs of interest} Even focusing on APIs that WebIDL marks, the total number of browser APIs exceeds 15,000 (depending in how you choose to categories inherited functions). We leverage Mozila Developer's network ~150 WebAPIs categories when aggregating all the APIs used. MDN will mark which specification's (for example Web Crypto, Fetch, or HTML DOM) modify the implementation of a given method call, constructor, or property access. This can result in `Window.fetch' falling both under `HTML DOM', `Fetch API', `Streams API', and `Attribution Reporting API'. To resolve this, we take in the \(\approx500\) APIs that have multiple mappings and resolve them into a single category based on our prior knowlage of browser functionality.
\input{figures/behavior_categories}
However, we also look at prior work by Su \etal{} and Zhang \etal{} for categories of Browser APIs for fingerprinting and general cloaking, respectively. We use MDN's documentation to identify a list of 22 APIs that trigger a permission pop-up or validate permission. We should note that while \textit{Permissions.query} is the only way to verify permissions, different WebAPIs like USB, clipboard, Geolocation, and Notification have their different APIs for requesting the pop-up. Combined with prior work, we-present a table mapping common phishing techniques to browser APIs in Table-\ref{tag:behaviorcategories}.

\myparagraph{De-duplicating kit files} We crawl the URLs with KitPhisher to establish a ground truth dataset with URLs originating from the same kit. For zip files extracted via KitPhisher that have password-based encryption enabled, we use the SHA256 hash of the zip files to identify which domains yielded the same kit. For the remaining zip files, we further de-duplicate them into Kit-Families by looking at them as a set of SHA256 of source files\footnote{Source files identified via libmagic}. If the Jaccard index-based similarity ($JI(A,B) = |A \cap B| / |A \cup B|$) of these sets is equal to or greater than 90\%, we consider the two kits to be from the same family, and thus group all domains from both kits into the same group.

\myparagraph{Adjusting for Cloudflare} We find a high number of anomalies originate from cloudflare scripts on the same domain as the phishing pages. Since cloudflare scripts load from a url with a \textbf{`cdn-cgi'} in the url, for most of our analysis we do not consider scripts loaded from that endpoint. We however, we do discuss the behaviors we were able to see from these scripts in Section-\ref{sec:results}.

\subsection{Identifying kits}

We hypothesize that similarity in browser API execution means that the pages originate from the same phishing kit. We use the Jaccard index on the API sets that first-party scripts execute to establish this similarity. We use HDBScan, with a minimum cluster size of 10, to cluster the pages based on the Jaccard index. We then use the ground truth labels from KitPhisher to evaluate the clustering. When ground truth is available, we evaluate the clustering using the Fowlkes-Mallows Index (geometric mean between precision and recall)\cite{fowlkes1983method} and V-measure.

V-measure is a harmonic mean between completeness (all members in a cluster are from the right class) and homogeneity (all members in a cluster are from the same class). V-measure allows tuning a ratio $\beta$ that prioritizes the score towards one vs the other. More importantly, looking at completeness and homogeneity scores lets you see how your clustering approach is getting things wrong\cite{Rosenberg2007VMeasureAC}.
We use sklearn's measure module to calculate all cluster evaluation metrics.\cite{scikit-learn}. For the much larger set of pages for which we do not have kit labels, we use silhouette score\footnote{While this metric is biased against non-convex clusters, based on our results, we do not see it necessary to switch to a density-specific cluster metric}, a metric for how well packed and separated the clusters are, to evaluate the clustering when we do not have ground truth. 

In order to process \totalPagesWithJavascriptFP{} pages in one go, we devide the pages into small time windows, and first cluster them into smaller local clusters. Figure-\ref{fig:infra} shows the breakdown of our methodology. But we first devide out data using a 3-week rolling window, we roll by 2 weeks. This creates duplicate pages across two clusters, so once we cluster these chunks using HDBSCAN, with no hyper-parameter tuning and a minimum size of 2, we merge any 2 clusters that have at least one page in common across the sliding window. We refer to these clusters as `local clusters'. However, these clusters can not represent phenomenons like re-emergence of clusters, if it happens after 2 weeks. To resolve this, we use the set-intersection of APIs from every page within a local-cluster, and use DBSCAN with a conservative $\epsilon=0.05$ in order to merge them together. Intuitivly, this $\epsilon$ represents the maxium dissimularity you are willing to tolorate between clusters. We present the clustering metrics from both local-clusters and final clusters in Section-\ref{sec:results}.

\subsection{Identifying trends}
\myparagraph{Identifying anomalies} While the bulk of this work is focused on identifiying phishing techniques as they are present in clusters of phishing pages, we also look at the overall anomalous patterns for a timeseries of every browser API we observe. We use rapture\cite{ruptures} to detect change points in the time series of API calls. We use a linearly penalized segmentation algorithm (PELT), which combines a penalty parameter $\beta$ and a cost function to quickly partition a time series based on points where it detects the series starts to change. For the cost function, we use \textbf{L1Cost}, which uses a deviation in the mean, and for the penalty parameter, we use $\beta=2 \ln{(n)=12}$ where $n$ is the number of days in the timeseries. This aligns with the recommendation of Killick \etal{} in the paper introducing PELT to minimize the Bayesian information criterion. We normalize by our daily URLs ingested and look at a time series of the percentage of daily URLs that executed the given URL. To clear up the results from rapture, we look at the change in the mean between segments. Any chance being less than 1\%, we ignore and merge with the prior segment. 