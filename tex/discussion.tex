\section{Discussion (1 page)}
\label{sec:discussion}
\input{figures/brand_vs_phishing.tex}
\subsection{Why clusters?}
As we have shown in the examples in this paper, phishing feeds can be a noisy data source for studying the overall ecosystem. From e-commerce pages to mass-spammed USPS and EZ-parking phishing pages, differentiating between a handful of pages of varying sizes becomes crucial for efficiency. This paper demonstrates that browser API usage can closely proxy the phishing kit (providing commonality for the page's origin) as best, and group via common client-side techniques as worst.

Furthermore, kit identification lets you infer server-side behavior about a particular page as long as a kit is identified and analyzed for one of the pages. As established by Oest\etal, Phishing kits thrive on including up-to-date IP blocklists. However, this blocklist is not always included in the client-side code. Being able to link pages to the same kit without any anti-evasion techniques like dynamic execution will allow better triaging of failed detections from phishing feed crawlers, something prior work established can be done by prior profiling~\cite{acharyaPhishPrintEvadingPhishing2021}.
\todowrite{I feel like this is where we gotta hammer home that AT WORST, you get a cluster of pages that all did the same thing using their javascript when you crawled the page (for example, Cloudflare captcha, cloacked away because of the user agent, use Squarespace for hosting, etc. etc...)}
\subsection{What makes a kit identifiable?}
Sets of browser APIs used are not the most granular method to describe pages; however, without many degrees of freedom in choices, the more sophisticated the phishing kit becomes, the easier it is to spot by just the browser APIs it uses. In plain JavaScript, without browser APIs, a page can not reach out to a C2 server, gather browser fingerprints, dynamically generate or cloak page contents, or request browser pop-ups without calling out to a browser API. In the case of the pages clustered in Figure-\ref{fig:ms_defender}, the APIs \textit{Keyboard.lock}, \textit{HTMLDocument.onkeydown} for keyboard locking, \textit{Window.atob} for obfuscation, and a handful of fingerprint APIs and DOM APIs for dynamic content generation, set these pages apart from the other clusters. 

\paperFinding{Phishing pages vary wildly from the brand that they are mimicking}
We collect browser API traces from Facebook, USPS, Meta, Microsoft, and IRS login pages and compare them to their phishing counterparts. The average similarity of the APIs executed by the phishing page and the original page is 11\%, indicating that browser APIs do not relate to the target page. We found no pages where the original page's API set was a subset of the phishing page's API set, and in 5\% of the cases, the phishing page executed at least half of the APIs from the original page. We report per-brand findings in Table-~\ref{tab:phishing-metrics} and note that the least similar brand was USPS, which could be the result of USPS phishing pages being multi-stage pages requiring user interaction and targeting credit card information~\cite{centerUSPSPhishingScam}.

This, however, does not indicate that browser API usage alone is a good indicator of maliciousness. All of the tehcniques described in Section-\ref{sec:methods} are common throughout the web, however, phishing pages are singular in their purpose, and their choise to engage in these techniques is what sets them apart from one another.

\subsection{Dynamic analysis combats evasions}
While dynamic analysis suffers from drawbacks we will discuss in Section-\ref{sec:limitations}, in JavaScript, it allows us to sidestep any sophisticated obfuscation techniques effectively. Even something as nuanced as an AES encrypted script will show up in VisisbleV8 as long as the behavior is triggered. While not incorporated by our work, prior work has shown forced execution ~\cite{fv8-sec24,crawlphish} to effectively ensure all behaviors are extracted by dynamic analysis.